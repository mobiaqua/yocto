diff --git a/drivers/gpu/drm/omapdrm/omap_drv.c b/drivers/gpu/drm/omapdrm/omap_drv.c
index 5c6609c..ebe307b 100644
--- a/drivers/gpu/drm/omapdrm/omap_drv.c
+++ b/drivers/gpu/drm/omapdrm/omap_drv.c
@@ -34,11 +34,20 @@
 #define DRIVER_MINOR		0
 #define DRIVER_PATCHLEVEL	0
 
+struct drm_device *drm_device;
+
 static int num_crtc = CONFIG_DRM_OMAP_NUM_CRTCS;
 
 MODULE_PARM_DESC(num_crtc, "Number of overlays to use as CRTCs");
 module_param(num_crtc, int, 0600);
 
+static struct omap_drm_plugin *sgx_plugin;
+
+/* keep track of whether we are already loaded.. we may need to call
+ * plugin's load() if they register after we are already loaded
+ */
+static bool loaded;
+
 /*
  * mode config funcs
  */
@@ -547,6 +556,19 @@ static int ioctl_set_param(struct drm_device *dev, void *data,
 	return 0;
 }
 
+static int ioctl_get_base(struct drm_device *dev, void *data,
+		struct drm_file *file_priv)
+{
+	struct drm_omap_get_base *args = data;
+
+	if (!sgx_plugin)
+		return -ENODEV;
+
+	args->ioctl_base = sgx_plugin->ioctl_base;
+
+	return 0;
+}
+
 static int ioctl_gem_new(struct drm_device *dev, void *data,
 		struct drm_file *file_priv)
 {
@@ -625,9 +647,10 @@ static int ioctl_gem_info(struct drm_device *dev, void *data,
 	return ret;
 }
 
-static const struct drm_ioctl_desc ioctls[DRM_COMMAND_END - DRM_COMMAND_BASE] = {
+static struct drm_ioctl_desc ioctls[DRM_COMMAND_END - DRM_COMMAND_BASE] = {
 	DRM_IOCTL_DEF_DRV(OMAP_GET_PARAM, ioctl_get_param, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(OMAP_SET_PARAM, ioctl_set_param, DRM_AUTH|DRM_MASTER|DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF_DRV(OMAP_GET_BASE, ioctl_get_base, DRM_UNLOCKED|DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(OMAP_GEM_NEW, ioctl_gem_new, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_PREP, ioctl_gem_cpu_prep, DRM_AUTH),
 	DRM_IOCTL_DEF_DRV(OMAP_GEM_CPU_FINI, ioctl_gem_cpu_fini, DRM_AUTH),
@@ -657,6 +680,8 @@ static int dev_load(struct drm_device *dev, unsigned long flags)
 
 	DBG("load: dev=%p", dev);
 
+	drm_device = dev;
+
 	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 	if (!priv)
 		return -ENOMEM;
@@ -702,6 +727,11 @@ static int dev_load(struct drm_device *dev, unsigned long flags)
 
 	drm_kms_helper_poll_init(dev);
 
+	loaded = true;
+
+	if (sgx_plugin)
+		sgx_plugin->load(dev, flags);
+
 	return 0;
 }
 
@@ -711,6 +741,9 @@ static int dev_unload(struct drm_device *dev)
 
 	DBG("unload: dev=%p", dev);
 
+	if (sgx_plugin)
+		sgx_plugin->unload(dev);
+
 	drm_kms_helper_poll_fini(dev);
 
 	if (priv->fbdev)
@@ -729,15 +762,18 @@ static int dev_unload(struct drm_device *dev)
 
 	dev_set_drvdata(dev->dev, NULL);
 
+	loaded = false;
+
 	return 0;
 }
 
 static int dev_open(struct drm_device *dev, struct drm_file *file)
 {
-	file->driver_priv = NULL;
-
 	DBG("open: dev=%p, file=%p", dev, file);
 
+	if (sgx_plugin)
+		sgx_plugin->open(dev, file);
+
 	return 0;
 }
 
@@ -794,6 +830,9 @@ static void dev_preclose(struct drm_device *dev, struct drm_file *file)
 
 	DBG("preclose: dev=%p", dev);
 
+	if (sgx_plugin)
+		sgx_plugin->release(dev, file);
+
 	/*
 	 * Unlink all pending CRTC events to make sure they won't be queued up
 	 * by a pending asynchronous commit.
@@ -806,6 +845,8 @@ static void dev_preclose(struct drm_device *dev, struct drm_file *file)
 		}
 	}
 	spin_unlock_irqrestore(&dev->event_lock, flags);
+
+	kfree(file->driver_priv);
 }
 
 static void dev_postclose(struct drm_device *dev, struct drm_file *file)
@@ -815,8 +856,8 @@ static void dev_postclose(struct drm_device *dev, struct drm_file *file)
 
 static const struct vm_operations_struct omap_gem_vm_ops = {
 	.fault = omap_gem_fault,
-	.open = drm_gem_vm_open,
-	.close = drm_gem_vm_close,
+	.open = omap_gem_vm_open,
+	.close = omap_gem_vm_close,
 };
 
 static const struct file_operations omapdriver_fops = {
@@ -866,6 +907,80 @@ static struct drm_driver omap_drm_driver = {
 	.patchlevel = DRIVER_PATCHLEVEL,
 };
 
+int omap_drm_register_plugin(struct omap_drm_plugin *plugin)
+{
+	struct drm_device *dev = drm_device;
+	int i;
+
+	DBG("register plugin: %p (%s)", plugin, plugin->name);
+
+	if (sgx_plugin)
+		return -EBUSY;
+
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		/* check for out of bounds ioctl or already registered ioctl */
+		if (nr > ARRAY_SIZE(ioctls) || ioctls[nr].func) {
+			dev_err(dev->dev, "invalid ioctl: %d (nr=%d)\n", i, nr);
+			return -EINVAL;
+		}
+	}
+
+	plugin->ioctl_base = DRM_OMAP_NUM_IOCTLS;
+
+	/* register the plugin's ioctl's */
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		DBG("register ioctl: %d %08x", nr, plugin->ioctls[i].cmd);
+
+		ioctls[nr] = plugin->ioctls[i];
+	}
+
+	omap_drm_driver.num_ioctls = DRM_OMAP_NUM_IOCTLS + plugin->num_ioctls;
+
+	sgx_plugin = plugin;
+
+	if (loaded)
+		plugin->load(dev, 0);
+
+	return 0;
+}
+EXPORT_SYMBOL(omap_drm_register_plugin);
+
+int omap_drm_unregister_plugin(struct omap_drm_plugin *plugin)
+{
+	int i;
+
+	for (i = 0; i < plugin->num_ioctls; i++) {
+		const struct drm_ioctl_desc empty = { 0 };
+		int nr = i + DRM_OMAP_NUM_IOCTLS;
+
+		ioctls[nr] = empty;
+	}
+
+	omap_drm_driver.num_ioctls = DRM_OMAP_NUM_IOCTLS;
+
+	sgx_plugin = NULL;
+
+	return 0;
+}
+EXPORT_SYMBOL(omap_drm_unregister_plugin);
+
+void *omap_drm_file_priv(struct drm_file *file)
+{
+	return file->driver_priv;
+}
+EXPORT_SYMBOL(omap_drm_file_priv);
+
+void omap_drm_file_set_priv(struct drm_file *file, void *priv)
+{
+	file->driver_priv = priv;
+}
+EXPORT_SYMBOL(omap_drm_file_set_priv);
+
+
 static int pdev_probe(struct platform_device *device)
 {
 	int r;
diff --git a/drivers/gpu/drm/omapdrm/omap_drv.h b/drivers/gpu/drm/omapdrm/omap_drv.h
index 5c367aa..48c9a7c 100644
--- a/drivers/gpu/drm/omapdrm/omap_drv.h
+++ b/drivers/gpu/drm/omapdrm/omap_drv.h
@@ -36,12 +36,6 @@
 
 #define MODULE_NAME     "omapdrm"
 
-/* max # of mapper-id's that can be assigned.. todo, come up with a better
- * (but still inexpensive) way to store/access per-buffer mapper private
- * data..
- */
-#define MAX_MAPPERS 2
-
 /* parameters which describe (unrotated) coordinates of scanout within a fb: */
 struct omap_drm_window {
 	uint32_t rotation;
@@ -268,4 +262,58 @@ fail:
 	return -ENOENT;
 }
 
+/****** PLUGIN API specific ******/
+
+/* interface that plug-in drivers (for now just PVR) can implement */
+struct omap_drm_plugin {
+	const char *name;
+
+	/* drm functions */
+	int (*load)(struct drm_device *dev, unsigned long flags);
+	int (*unload)(struct drm_device *dev);
+	int (*open)(struct drm_device *dev, struct drm_file *file);
+	int (*release)(struct drm_device *dev, struct drm_file *file);
+
+	struct drm_ioctl_desc *ioctls;
+	int num_ioctls;
+	int ioctl_base;
+};
+
+int omap_drm_register_plugin(struct omap_drm_plugin *plugin);
+int omap_drm_unregister_plugin(struct omap_drm_plugin *plugin);
+
+void *omap_drm_file_priv(struct drm_file *file);
+void omap_drm_file_set_priv(struct drm_file *file, void *priv);
+
+void *omap_gem_priv(struct drm_gem_object *obj);
+void omap_gem_set_priv(struct drm_gem_object *obj, void *priv);
+void omap_gem_vm_open(struct vm_area_struct *vma);
+void omap_gem_vm_close(struct vm_area_struct *vma);
+
+/* for external plugin buffers wrapped as GEM object (via. omap_gem_new_ext())
+ * a vm_ops struct can be provided to get callback notification of various
+ * events..
+ */
+struct omap_gem_vm_ops {
+	void (*open)(struct vm_area_struct *area);
+	void (*close)(struct vm_area_struct *area);
+	/*maybe: int (*fault)(struct vm_area_struct *vma,
+	  struct vm_fault *vmf)*/
+
+	/* note: mmap isn't expected to do anything. it is just to allow buffer
+	 * allocate to update it's own internal state
+	 */
+	void (*mmap)(struct file *, struct vm_area_struct *);
+};
+
+struct drm_gem_object *omap_gem_new_ext(struct drm_device *dev,
+		union omap_gem_size gsize, uint32_t flags,
+		dma_addr_t paddr, struct page **pages,
+		struct omap_gem_vm_ops *ops);
+
+void omap_gem_op_update(void);
+int omap_gem_set_sync_object(struct drm_gem_object *obj, void *syncobj);
+/*********************************/
+
+
 #endif /* __OMAP_DRV_H__ */
diff --git a/drivers/gpu/drm/omapdrm/omap_gem.c b/drivers/gpu/drm/omapdrm/omap_gem.c
index 393e533..c495c27 100644
--- a/drivers/gpu/drm/omapdrm/omap_gem.c
+++ b/drivers/gpu/drm/omapdrm/omap_gem.c
@@ -117,6 +117,14 @@ struct omap_gem_object {
 		uint32_t read_pending;
 		uint32_t read_complete;
 	} *sync;
+
+	struct omap_gem_vm_ops *ops;
+
+	/**
+	 * per-mapper private data..
+	 */
+	void *priv;
+
 };
 
 static int get_pages(struct drm_gem_object *obj, struct page ***pages);
@@ -300,6 +308,7 @@ uint32_t omap_gem_flags(struct drm_gem_object *obj)
 {
 	return to_omap_bo(obj)->flags;
 }
+EXPORT_SYMBOL(omap_gem_flags);
 
 /** get mmap offset */
 static uint64_t mmap_offset(struct drm_gem_object *obj)
@@ -329,6 +338,7 @@ uint64_t omap_gem_mmap_offset(struct drm_gem_object *obj)
 	mutex_unlock(&obj->dev->struct_mutex);
 	return offset;
 }
+EXPORT_SYMBOL(omap_gem_mmap_offset);
 
 /** get mmap size */
 size_t omap_gem_mmap_size(struct drm_gem_object *obj)
@@ -361,6 +371,7 @@ int omap_gem_tiled_size(struct drm_gem_object *obj, uint16_t *w, uint16_t *h)
 	}
 	return -EINVAL;
 }
+EXPORT_SYMBOL(omap_gem_tiled_size);
 
 /* Normal handling for the case of faulting in non-tiled buffers */
 static int fault_1d(struct drm_gem_object *obj,
@@ -593,6 +604,9 @@ int omap_gem_mmap_obj(struct drm_gem_object *obj,
 		vma->vm_page_prot = vm_get_page_prot(vma->vm_flags);
 	}
 
+	if (omap_obj->ops && omap_obj->ops->mmap)
+		omap_obj->ops->mmap(obj->filp, vma);
+
 	return 0;
 }
 
@@ -804,6 +818,7 @@ fail:
 
 	return ret;
 }
+EXPORT_SYMBOL(omap_gem_get_paddr);
 
 /* Release physical address, when DMA is no longer being performed.. this
  * could potentially unpin and unmap buffers from TILER
@@ -834,6 +849,7 @@ void omap_gem_put_paddr(struct drm_gem_object *obj)
 
 	mutex_unlock(&obj->dev->struct_mutex);
 }
+EXPORT_SYMBOL(omap_gem_put_paddr);
 
 /* Get rotated scanout address (only valid if already pinned), at the
  * specified orientation and x,y offset from top-left corner of buffer
@@ -864,6 +880,7 @@ int omap_gem_tiled_stride(struct drm_gem_object *obj, uint32_t orient)
 		ret = tiler_stride(gem2fmt(omap_obj->flags), orient);
 	return ret;
 }
+EXPORT_SYMBOL(omap_gem_tiled_stride);
 
 /* acquire pages when needed (for example, for DMA where physically
  * contiguous buffer is not required
@@ -913,6 +930,7 @@ int omap_gem_get_pages(struct drm_gem_object *obj, struct page ***pages,
 	mutex_unlock(&obj->dev->struct_mutex);
 	return ret;
 }
+EXPORT_SYMBOL(omap_gem_get_pages);
 
 /* release pages when DMA no longer being performed */
 int omap_gem_put_pages(struct drm_gem_object *obj)
@@ -923,6 +941,7 @@ int omap_gem_put_pages(struct drm_gem_object *obj)
 	 */
 	return 0;
 }
+EXPORT_SYMBOL(omap_gem_put_pages);
 
 /* Get kernel virtual address for CPU access.. this more or less only
  * exists for omap_fbdev.  This should be called with struct_mutex
@@ -1118,17 +1137,20 @@ void omap_gem_op_update(void)
 	sync_op_update();
 	spin_unlock(&sync_lock);
 }
+EXPORT_SYMBOL(omap_gem_op_update);
 
 /* mark the start of read and/or write operation */
 int omap_gem_op_start(struct drm_gem_object *obj, enum omap_gem_op op)
 {
 	return sync_op(obj, op, true);
 }
+EXPORT_SYMBOL(omap_gem_op_start);
 
 int omap_gem_op_finish(struct drm_gem_object *obj, enum omap_gem_op op)
 {
 	return sync_op(obj, op, false);
 }
+EXPORT_SYMBOL(omap_gem_op_finish);
 
 static DECLARE_WAIT_QUEUE_HEAD(sync_event);
 
@@ -1181,6 +1203,7 @@ int omap_gem_op_sync(struct drm_gem_object *obj, enum omap_gem_op op)
 	}
 	return ret;
 }
+EXPORT_SYMBOL(omap_gem_op_sync);
 
 /* call fxn(arg), either synchronously or asynchronously if the op
  * is currently blocked..  fxn() can be called from any context
@@ -1227,6 +1250,7 @@ int omap_gem_op_async(struct drm_gem_object *obj, enum omap_gem_op op,
 
 	return 0;
 }
+EXPORT_SYMBOL(omap_gem_op_async);
 
 /* special API so PVR can update the buffer to use a sync-object allocated
  * from it's sync-obj heap.  Only used for a newly allocated (from PVR's
@@ -1264,6 +1288,7 @@ unlock:
 	spin_unlock(&sync_lock);
 	return ret;
 }
+EXPORT_SYMBOL(omap_gem_set_sync_object);
 
 /* don't call directly.. called from GEM core when it is time to actually
  * free the object..
@@ -1487,3 +1512,68 @@ void omap_gem_deinit(struct drm_device *dev)
 	 */
 	kfree(usergart);
 }
+
+/****** PLUGIN API specific ******/
+
+/* This constructor is mainly to give plugins a way to wrap their
+ * own allocations
+ */
+struct drm_gem_object *omap_gem_new_ext(struct drm_device *dev,
+		union omap_gem_size gsize, uint32_t flags,
+		dma_addr_t paddr, struct page **pages,
+		struct omap_gem_vm_ops *ops)
+{
+	struct drm_gem_object *obj;
+
+	BUG_ON((flags & OMAP_BO_TILED) && !pages);
+
+	if (paddr)
+		flags |= OMAP_BO_DMA;
+
+	obj = omap_gem_new(dev, gsize, flags | OMAP_BO_EXT_MEM);
+	if (obj) {
+		struct omap_gem_object *omap_obj = to_omap_bo(obj);
+
+		omap_obj->paddr = paddr;
+		omap_obj->pages = pages;
+		omap_obj->ops = ops;
+	}
+	return obj;
+}
+EXPORT_SYMBOL(omap_gem_new_ext);
+
+void omap_gem_vm_open(struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+
+	if (omap_obj->ops && omap_obj->ops->open)
+		omap_obj->ops->open(vma);
+	else
+		drm_gem_vm_open(vma);
+
+}
+
+void omap_gem_vm_close(struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = vma->vm_private_data;
+	struct omap_gem_object *omap_obj = to_omap_bo(obj);
+
+	if (omap_obj->ops && omap_obj->ops->close)
+		omap_obj->ops->close(vma);
+	else
+		drm_gem_vm_close(vma);
+
+}
+
+void *omap_gem_priv(struct drm_gem_object *obj)
+{
+	return to_omap_bo(obj)->priv;
+}
+EXPORT_SYMBOL(omap_gem_priv);
+
+void omap_gem_set_priv(struct drm_gem_object *obj, void *priv)
+{
+	to_omap_bo(obj)->priv = priv;
+}
+EXPORT_SYMBOL(omap_gem_set_priv);
diff --git a/include/uapi/drm/omap_drm.h b/include/uapi/drm/omap_drm.h
index 1d0b117..5292b93 100644
--- a/include/uapi/drm/omap_drm.h
+++ b/include/uapi/drm/omap_drm.h
@@ -33,6 +33,12 @@ struct drm_omap_param {
 	uint64_t value;			/* in (set_param), out (get_param) */
 };
 
+struct drm_omap_get_base {
+	char plugin_name[64];           /* in */
+	uint32_t ioctl_base;            /* out */
+	uint32_t __pad;
+};
+
 #define OMAP_BO_SCANOUT		0x00000001	/* scanout capable (phys contiguous) */
 #define OMAP_BO_CACHE_MASK	0x00000006	/* cache type mask, see cache modes */
 #define OMAP_BO_TILED_MASK	0x00000f00	/* tiled mapping mask, see tiled modes */
@@ -101,9 +107,7 @@ struct drm_omap_gem_info {
 
 #define DRM_OMAP_GET_PARAM		0x00
 #define DRM_OMAP_SET_PARAM		0x01
-/* placeholder for plugin-api
 #define DRM_OMAP_GET_BASE		0x02
-*/
 #define DRM_OMAP_GEM_NEW		0x03
 #define DRM_OMAP_GEM_CPU_PREP		0x04
 #define DRM_OMAP_GEM_CPU_FINI		0x05
@@ -112,9 +116,7 @@ struct drm_omap_gem_info {
 
 #define DRM_IOCTL_OMAP_GET_PARAM	DRM_IOWR(DRM_COMMAND_BASE + DRM_OMAP_GET_PARAM, struct drm_omap_param)
 #define DRM_IOCTL_OMAP_SET_PARAM	DRM_IOW (DRM_COMMAND_BASE + DRM_OMAP_SET_PARAM, struct drm_omap_param)
-/* placeholder for plugin-api
 #define DRM_IOCTL_OMAP_GET_BASE		DRM_IOWR(DRM_COMMAND_BASE + DRM_OMAP_GET_BASE, struct drm_omap_get_base)
-*/
 #define DRM_IOCTL_OMAP_GEM_NEW		DRM_IOWR(DRM_COMMAND_BASE + DRM_OMAP_GEM_NEW, struct drm_omap_gem_new)
 #define DRM_IOCTL_OMAP_GEM_CPU_PREP	DRM_IOW (DRM_COMMAND_BASE + DRM_OMAP_GEM_CPU_PREP, struct drm_omap_gem_cpu_prep)
 #define DRM_IOCTL_OMAP_GEM_CPU_FINI	DRM_IOW (DRM_COMMAND_BASE + DRM_OMAP_GEM_CPU_FINI, struct drm_omap_gem_cpu_fini)
