From 8a9c8625744b2049c43216d1ff49060dfde7b643 Mon Sep 17 00:00:00 2001
From: Pratyush Yadav <p.yadav@ti.com>
Date: Fri, 21 Apr 2023 11:33:47 +0530
Subject: [PATCH 320/508] media: ti: j721e-csi2rx: Enable DMA draining

Once frames start dropping due to lack of buffers, some data gets stuck
in the DMA pipeline somewhere. So the first DMA transfer after frame
drops gives a partial frame. This is obviously not useful to the
application and will only serve to confuse it. Issue a DMA transaction
to drain that up so that the next frame comes out clean.

Similarly when stopping a stream, the DMA terminate does not currently
ensure a graceful cleanup of the pending data in the fabric. Due to this
pending packet affecting the DMA framing, other contexts that are
streaming will also be interrupted as the CSI_RX_IF expects previous
packets to be fully consumed before any new line, end of frame or start
of frame can be sent. Thus perform ti_csi2rx_drain_dma() to drain any
pending data, also update the log level for the drain DMA failure from
dev_warn to dev_dbg during stop, even when this warning was seen, the
next frame was proper this could be due to the fact that DMACNTX is
disabled asynchronously and the pending data is shorter.

Signed-off-by: Pratyush Yadav <p.yadav@ti.com>
[Drain DMA during stream stop]
Signed-off-by: Vaishnav Achath <vaishnav.a@ti.com>
Signed-off-by: Jai Luthra <j-luthra@ti.com>
---
 .../platform/ti/j721e-csi2rx/j721e-csi2rx.c   | 113 ++++++++++++++++--
 1 file changed, 101 insertions(+), 12 deletions(-)

diff --git a/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c b/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
index f367e8e9422d..aba3efd0d4d6 100644
--- a/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
+++ b/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
@@ -58,6 +58,8 @@
 #define TI_CSI2RX_MAX_SOURCE_PADS	TI_CSI2RX_MAX_CTX
 #define TI_CSI2RX_MAX_PADS		(1 + TI_CSI2RX_MAX_SOURCE_PADS)
 
+#define DRAIN_TIMEOUT_MS		50
+
 struct ti_csi2rx_fmt {
 	u32				fourcc;	/* Four character code. */
 	u32				code;	/* Mbus code. */
@@ -489,6 +491,59 @@ static void ti_csi2rx_setup_shim(struct ti_csi2rx_ctx *ctx)
 	writel(reg, csi->shim + SHIM_PSI_CFG0(ctx->idx));
 }
 
+static void ti_csi2rx_drain_callback(void *param)
+{
+	struct completion *drain_complete = param;
+
+	complete(drain_complete);
+}
+
+static int ti_csi2rx_drain_dma(struct ti_csi2rx_ctx *csi)
+{
+	struct dma_async_tx_descriptor *desc;
+	struct device *dev = csi->dma.chan->device->dev;
+	struct completion drain_complete;
+	void *buf;
+	size_t len = csi->v_fmt.fmt.pix.sizeimage;
+	dma_addr_t addr;
+	dma_cookie_t cookie;
+	int ret;
+
+	init_completion(&drain_complete);
+
+	buf = dma_alloc_coherent(dev, len, &addr, GFP_KERNEL | GFP_ATOMIC);
+	if (!buf)
+		return -ENOMEM;
+
+	desc = dmaengine_prep_slave_single(csi->dma.chan, addr, len,
+					   DMA_DEV_TO_MEM,
+					   DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!desc) {
+		ret = -EIO;
+		goto out;
+	}
+
+	desc->callback = ti_csi2rx_drain_callback;
+	desc->callback_param = &drain_complete;
+
+	cookie = dmaengine_submit(desc);
+	ret = dma_submit_error(cookie);
+	if (ret)
+		goto out;
+
+	dma_async_issue_pending(csi->dma.chan);
+
+	if (!wait_for_completion_timeout(&drain_complete,
+					 msecs_to_jiffies(DRAIN_TIMEOUT_MS))) {
+		dmaengine_terminate_sync(csi->dma.chan);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+out:
+	dma_free_coherent(dev, len, buf, addr);
+	return ret;
+}
+
 static void ti_csi2rx_dma_callback(void *param)
 {
 	struct ti_csi2rx_buffer *buf = param;
@@ -560,24 +615,35 @@ static int ti_csi2rx_start_dma(struct ti_csi2rx_ctx *ctx,
 }
 
 static void ti_csi2rx_cleanup_buffers(struct ti_csi2rx_ctx *ctx,
-				      enum vb2_buffer_state state)
+				      enum vb2_buffer_state buf_state)
 {
 	struct ti_csi2rx_dma *dma = &ctx->dma;
 	struct ti_csi2rx_buffer *buf = NULL, *tmp;
+	enum ti_csi2rx_dma_state state;
 	unsigned long flags;
+	int ret;
 
 	spin_lock_irqsave(&dma->lock, flags);
 	list_for_each_entry_safe(buf, tmp, &ctx->dma.queue, list) {
 		list_del(&buf->list);
-		vb2_buffer_done(&buf->vb.vb2_buf, state);
+		vb2_buffer_done(&buf->vb.vb2_buf, buf_state);
 	}
 
 	if (dma->curr)
-		vb2_buffer_done(&dma->curr->vb.vb2_buf, state);
+		vb2_buffer_done(&dma->curr->vb.vb2_buf, buf_state);
+
+	state = ctx->dma.state;
 
 	dma->curr = NULL;
 	dma->state = TI_CSI2RX_DMA_STOPPED;
 	spin_unlock_irqrestore(&dma->lock, flags);
+
+	if (state != TI_CSI2RX_DMA_STOPPED) {
+		ret = ti_csi2rx_drain_dma(ctx);
+		if (ret)
+			dev_dbg(ctx->csi->dev,
+				"Failed to drain DMA. Next frame might be bogus\n");
+	}
 }
 
 static int ti_csi2rx_queue_setup(struct vb2_queue *q, unsigned int *nbuffers,
@@ -618,6 +684,7 @@ static void ti_csi2rx_buffer_queue(struct vb2_buffer *vb)
 	struct ti_csi2rx_ctx *ctx = vb2_get_drv_priv(vb->vb2_queue);
 	struct ti_csi2rx_buffer *buf;
 	struct ti_csi2rx_dma *dma = &ctx->dma;
+	bool restart_dma = false;
 	unsigned long flags = 0;
 	int ret;
 
@@ -630,21 +697,43 @@ static void ti_csi2rx_buffer_queue(struct vb2_buffer *vb)
 	 * But if DMA has stalled due to lack of buffers, restart it now.
 	 */
 	if (dma->state == TI_CSI2RX_DMA_IDLE) {
-		ret = ti_csi2rx_start_dma(ctx, buf);
-		if (ret) {
-			dev_err(ctx->csi->dev, "Failed to start DMA: %d\n", ret);
-			vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_QUEUED);
-			goto unlock;
-		}
-
+		/*
+		 * Do not restart DMA with the lock held because
+		 * ti_csi2rx_drain_dma() might block when allocating a buffer.
+		 * There won't be a race on queueing DMA anyway since the
+		 * callback is not being fired.
+		 */
+		restart_dma = true;
 		dma->curr = buf;
 		dma->state = TI_CSI2RX_DMA_ACTIVE;
 	} else {
 		list_add_tail(&buf->list, &dma->queue);
 	}
-
-unlock:
 	spin_unlock_irqrestore(&dma->lock, flags);
+
+	if (restart_dma) {
+		/*
+		 * Once frames start dropping, some data gets stuck in the DMA
+		 * pipeline somewhere. So the first DMA transfer after frame
+		 * drops gives a partial frame. This is obviously not useful to
+		 * the application and will only confuse it. Issue a DMA
+		 * transaction to drain that up.
+		 */
+		ret = ti_csi2rx_drain_dma(ctx);
+		if (ret)
+			dev_warn(ctx->csi->dev,
+				 "Failed to drain DMA. Next frame might be bogus\n");
+
+		ret = ti_csi2rx_start_dma(ctx, buf);
+		if (ret) {
+			dev_err(ctx->csi->dev, "Failed to start DMA: %d\n", ret);
+			spin_lock_irqsave(&dma->lock, flags);
+			vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);
+			dma->curr = NULL;
+			dma->state = TI_CSI2RX_DMA_IDLE;
+			spin_unlock_irqrestore(&dma->lock, flags);
+		}
+	}
 }
 
 static int ti_csi2rx_start_streaming(struct vb2_queue *vq, unsigned int count)
-- 
2.41.0

